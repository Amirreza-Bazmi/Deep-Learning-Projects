{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **lib**"
      ],
      "metadata": {
        "id": "hVlA86pSppG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "!pip install hazm\n",
        "import hazm\n",
        "!pip install plotly\n",
        "import plotly.graph_objects as go\n",
        "import re\n",
        "!pip install clean-text\n",
        "from cleantext import clean"
      ],
      "metadata": {
        "id": "WiHOt3S9pQke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Distribution and Summary without Preprocessing**"
      ],
      "metadata": {
        "id": "6NSZGUG7qI6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def distribution(df):\n",
        "  # Plot the distribution of classes (labels)\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  sns.countplot(x='label', data=df)\n",
        "  plt.title('Distribution of Classes')\n",
        "  plt.xlabel('Class')\n",
        "  plt.ylabel('Count')\n",
        "  plt.show()\n",
        "\n",
        "  # Tokenize sentences and calculate the length (in terms of tokens)\n",
        "  df['premise_length'] = df['premise'].apply(lambda x: len(str(x).split()))\n",
        "  df['hypothesis_length'] = df['hypothesis'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "  # Plot the distribution of sentence lengths for premise and hypothesis\n",
        "  plt.figure(figsize=(12, 6))\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  sns.histplot(df['premise_length'], bins=30, kde=True)\n",
        "  plt.title('Distribution of Premise Sentence Lengths')\n",
        "  plt.xlabel('Number of Tokens')\n",
        "  plt.ylabel('Count')\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  sns.histplot(df['hypothesis_length'], bins=30, kde=True)\n",
        "  plt.title('Distribution of Hypothesis Sentence Lengths')\n",
        "  plt.xlabel('Number of Tokens')\n",
        "  plt.ylabel('Count')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "def summary(df):\n",
        "  # Summary statistics for sentence lengths\n",
        "  premise_length_stats = df['premise_length'].describe()\n",
        "  hypothesis_length_stats = df['hypothesis_length'].describe()\n",
        "\n",
        "  print(\"Premise Length Statistics:\\n\", premise_length_stats)\n",
        "  print(\"Hypothesis Length Statistics:\\n\", hypothesis_length_stats)\n",
        "\n"
      ],
      "metadata": {
        "id": "aUByLVK3pVR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preproccing Function**"
      ],
      "metadata": {
        "id": "Dc_utEqnqRU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Preprocessing\n",
        "def cleanhtml(raw_html):\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, '', raw_html)\n",
        "    return cleantext\n",
        "\n",
        "\n",
        "def cleaning(text):\n",
        "    text = text.strip()\n",
        "\n",
        "    # regular cleaning\n",
        "    text = clean(text,\n",
        "        fix_unicode=True,\n",
        "        to_ascii=False,\n",
        "        lower=True,\n",
        "        no_line_breaks=True,\n",
        "        no_urls=True,\n",
        "        no_emails=True,\n",
        "        no_phone_numbers=True,\n",
        "        no_numbers=False,\n",
        "        no_digits=False,\n",
        "        no_currency_symbols=True,\n",
        "        no_punct=False,\n",
        "        replace_with_url=\"\",\n",
        "        replace_with_email=\"\",\n",
        "        replace_with_phone_number=\"\",\n",
        "        replace_with_number=\"\",\n",
        "        replace_with_digit=\"0\",\n",
        "        replace_with_currency_symbol=\"\",\n",
        "    )\n",
        "\n",
        "    # cleaning htmls\n",
        "    text = cleanhtml(text)\n",
        "\n",
        "    # normalizing\n",
        "    # normalizer = hazm.Normalizer()\n",
        "    # text = normalizer.normalize(text)\n",
        "\n",
        "    # removing wierd patterns\n",
        "    wierd_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u'\\U00010000-\\U0010ffff'\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\u3030\"\n",
        "        u\"\\ufe0f\"\n",
        "        u\"\\u2069\"\n",
        "        u\"\\u2066\"\n",
        "        # u\"\\u200c\"\n",
        "        u\"\\u2068\"\n",
        "        u\"\\u2067\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "\n",
        "    text = wierd_pattern.sub(r'', text)\n",
        "\n",
        "    # removing extra spaces, hashtags\n",
        "    text = re.sub(\"#\", \"\", text)\n",
        "    text = re.sub(\"\\s+\", \" \", text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def preprocess(df):\n",
        "    data = df[['premise', \"hypothesis\", \"label\"]]\n",
        "\n",
        "    # print data information\n",
        "    print('data information')\n",
        "    print(df.info(), '\\n')\n",
        "\n",
        "    # print missing values information\n",
        "    print('missing values stats')\n",
        "    print(data.isnull().sum(), '\\n')\n",
        "\n",
        "    # print some missing values\n",
        "    print('some missing values')\n",
        "    print(data[data['premise'].isnull()].iloc[:5], '\\n')\n",
        "    print(data[data['hypothesis'].isnull()].iloc[:5], '\\n')\n",
        "    print(data[data['label'].isnull()].iloc[:5], '\\n')\n",
        "\n",
        "\n",
        "    data = data.dropna(subset=['premise'])\n",
        "    data = data.dropna(subset=['hypothesis'])\n",
        "    data = data.dropna(subset=['label'])\n",
        "    data = data.drop_duplicates(subset=['premise'], keep='first')\n",
        "    data = data.drop_duplicates(subset=['hypothesis'], keep='first')\n",
        "    data = data.reset_index(drop=True)\n",
        "\n",
        "\n",
        "    # print data information\n",
        "    print('data information')\n",
        "    print(data.info(), '\\n')\n",
        "\n",
        "    # print missing values information\n",
        "    print('missing values stats')\n",
        "    print(data.isnull().sum(), '\\n')\n",
        "\n",
        "\n",
        "    # print some missing values\n",
        "    print('some missing values')\n",
        "    print(data[data['premise'].isnull()].iloc[:5], '\\n')\n",
        "\n",
        "    # Normalization\n",
        "    # calculate the length based on their words\n",
        "    data['premise_len_by_words'] = data['premise'].apply(lambda t: len(hazm.word_tokenize(t)))\n",
        "    data['hypothesis_len_by_words'] = data['hypothesis'].apply(lambda t: len(hazm.word_tokenize(t)))\n",
        "\n",
        "\n",
        "    min_max_len = data[\"premise_len_by_words\"].min(), data[\"premise_len_by_words\"].max()\n",
        "    print(f'premise:\\nMin: {min_max_len[0]} \\tMax: {min_max_len[1]}')\n",
        "\n",
        "    min_max_len = data[\"hypothesis_len_by_words\"].min(), data[\"hypothesis_len_by_words\"].max()\n",
        "    print(f'hypothesis:\\nMin: {min_max_len[0]} \\tMax: {min_max_len[1]}')\n",
        "\n",
        "    minlim, maxlim = 3, 256\n",
        "\n",
        "    # remove comments with the length of fewer than three words\n",
        "    data['premise_len_by_words'] = data['premise_len_by_words'].apply(lambda len_t: len_t if minlim < len_t <= maxlim else None)\n",
        "    data = data.dropna(subset=['premise_len_by_words'])\n",
        "    data = data.reset_index(drop=True)\n",
        "\n",
        "    data['hypothesis_len_by_words'] = data['hypothesis_len_by_words'].apply(lambda len_t: len_t if minlim < len_t <= maxlim else None)\n",
        "    data = data.dropna(subset=['hypothesis_len_by_words'])\n",
        "    data = data.reset_index(drop=True)\n",
        "\n",
        "    fig = go.Figure()\n",
        "\n",
        "    fig.add_trace(go.Histogram(\n",
        "    x=data['premise_len_by_words']))\n",
        "\n",
        "    fig.update_layout(\n",
        "      title_text='Distribution of word counts for premise',\n",
        "      xaxis_title_text='Word Count',\n",
        "      yaxis_title_text='Frequency',\n",
        "      bargap=0.2,\n",
        "      bargroupgap=0.2)\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "    fig = go.Figure()\n",
        "\n",
        "    fig.add_trace(go.Histogram(\n",
        "    x=data['hypothesis_len_by_words']))\n",
        "\n",
        "    fig.update_layout(\n",
        "      title_text='Distribution of word counts for hypothesis',\n",
        "      xaxis_title_text='Word Count',\n",
        "      yaxis_title_text='Frequency',\n",
        "      bargap=0.2,\n",
        "      bargroupgap=0.2)\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "    # uniques\n",
        "    unique_rates = list(sorted(data['premise'].unique()))\n",
        "    print(f'We have #{len(unique_rates)}: {unique_rates}')\n",
        "\n",
        "    unique_rates = list(sorted(data['hypothesis'].unique()))\n",
        "    print(f'We have #{len(unique_rates)}: {unique_rates}')\n",
        "\n",
        "\n",
        "    fig = go.Figure()\n",
        "\n",
        "    groupby_rate = data.groupby('premise')['premise'].count()\n",
        "\n",
        "    fig.add_trace(go.Bar(\n",
        "        x=list(sorted(groupby_rate.index)),\n",
        "        y=groupby_rate.tolist(),\n",
        "        text=groupby_rate.tolist(),\n",
        "        textposition='auto'\n",
        "    ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title_text='Distribution of rate within comments',\n",
        "        xaxis_title_text='Rate',\n",
        "        yaxis_title_text='Frequency',\n",
        "        bargap=0.2,\n",
        "        bargroupgap=0.2)\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "    fig = go.Figure()\n",
        "\n",
        "    groupby_rate = data.groupby('hypothesis')['hypothesis'].count()\n",
        "\n",
        "    fig.add_trace(go.Bar(\n",
        "        x=list(sorted(groupby_rate.index)),\n",
        "        y=groupby_rate.tolist(),\n",
        "        text=groupby_rate.tolist(),\n",
        "        textposition='auto'\n",
        "    ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title_text='Distribution of rate within comments',\n",
        "        xaxis_title_text='Rate',\n",
        "        yaxis_title_text='Frequency',\n",
        "        bargap=0.2,\n",
        "        bargroupgap=0.2)\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "    print(\"Cleaning premise...\")\n",
        "    # cleaning premise\n",
        "    data['cleaned_premise'] = data['premise'].apply(cleaning)\n",
        "\n",
        "\n",
        "    # calculate the length of premise based on their words\n",
        "    data['cleaned_premise_len_by_words'] = data['cleaned_premise'].apply(lambda t: len(hazm.word_tokenize(t)))\n",
        "\n",
        "    # remove premise with the length of fewer than three words\n",
        "    data['cleaned_premise_len_by_words'] = data['cleaned_premise_len_by_words'].apply(lambda len_t: len_t if minlim < len_t <= maxlim else len_t)\n",
        "    data = data.dropna(subset=['cleaned_premise_len_by_words'])\n",
        "    data = data.reset_index(drop=True)\n",
        "\n",
        "    # data.head()\n",
        "\n",
        "    print(\"Cleaning hypothesis ...\")\n",
        "    # cleaning hypothesis\n",
        "    data['cleaned_hypothesis'] = data['hypothesis'].apply(cleaning)\n",
        "\n",
        "\n",
        "    # calculate the length of hypothesis based on their words\n",
        "    data['cleaned_hypothesis_len_by_words'] = data['cleaned_hypothesis'].apply(lambda t: len(hazm.word_tokenize(t)))\n",
        "\n",
        "    # remove premise with the length of fewer than three words\n",
        "    data['cleaned_hypothesis_len_by_words'] = data['cleaned_hypothesis_len_by_words'].apply(lambda len_t: len_t if minlim < len_t <= maxlim else len_t)\n",
        "    data = data.dropna(subset=['cleaned_hypothesis_len_by_words'])\n",
        "    data = data.reset_index(drop=True)\n",
        "\n",
        "    data.head()\n",
        "\n",
        "    # Create new dataframe\n",
        "    data = data[['cleaned_premise', 'cleaned_hypothesis','label']]\n",
        "    # data = data[['premise', 'hypothesis','label']]\n",
        "    data.columns = ['premise', 'hypothesis', 'label']\n",
        "    data.head()\n",
        "\n",
        "    # Handling Unbalanced Data\n",
        "\n",
        "    fig = go.Figure()\n",
        "\n",
        "    groupby_label = data.groupby('label')['label'].count()\n",
        "\n",
        "    fig.add_trace(go.Bar(\n",
        "        x=list(sorted(groupby_label.index)),\n",
        "        y=groupby_label.tolist(),\n",
        "        text=groupby_label.tolist(),\n",
        "        textposition='auto'\n",
        "    ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title_text='Distribution of label within comments [DATA]',\n",
        "        xaxis_title_text='Label',\n",
        "        yaxis_title_text='Frequency',\n",
        "        bargap=0.2,\n",
        "        bargroupgap=0.2)\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "swl-m1ZxpYCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing Train Dataset**"
      ],
      "metadata": {
        "id": "g8Iw3WzxqYMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "# Train\n",
        "df_train = pd.read_csv('Train-word.csv', delimiter='\\t')\n",
        "print(df_train.info())\n",
        "distribution(df_train)\n",
        "summary(df_train)\n",
        "data_train = preprocess(df_train)\n",
        "data_train.to_csv(\"cleaned_train.csv\", index = False)"
      ],
      "metadata": {
        "id": "XlMPIgtqpap-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing Test Dataset**"
      ],
      "metadata": {
        "id": "iainzZPnqf-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Test\n",
        "df_test = pd.read_csv('Test-word.csv', delimiter='\\t')\n",
        "print(df_test.info())\n",
        "distribution(df_test)\n",
        "summary(df_test)\n",
        "data_test = preprocess(df_test)\n",
        "data_test.to_csv(\"cleaned_test.csv\", index = False)"
      ],
      "metadata": {
        "id": "qZBg9OVSpc_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing Validation Dataset**"
      ],
      "metadata": {
        "id": "-Rq1ucMFqkHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Val\n",
        "df_val = pd.read_csv('Val-word.csv', delimiter='\\t')\n",
        "print(df_val.info())\n",
        "distribution(df_val)\n",
        "summary(df_val)\n",
        "data_val = preprocess(df_val)\n",
        "data_val.to_csv(\"cleaned_val.csv\", index = False)"
      ],
      "metadata": {
        "id": "zVmhaicupfh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fine Tuning the ParsBERT**"
      ],
      "metadata": {
        "id": "v3A7ff7kqnF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers torch datasets"
      ],
      "metadata": {
        "id": "1fwRxDSlpiB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
        "from datasets import Dataset, DatasetDict\n",
        "import torch\n",
        "\n",
        "# Load the tokenizer and model\n",
        "model_name = 'HooshvareLab/bert-base-parsbert-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "# Read the CSV files\n",
        "train_df = pd.read_csv('cleaned_train.csv')\n",
        "val_df = pd.read_csv('cleaned_val.csv')\n",
        "test_df = pd.read_csv('cleaned_test.csv')\n",
        "\n",
        "# Convert DataFrame to Dataset\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# Function to tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['premise'], examples['hypothesis'], padding=True, truncation=True)\n",
        "\n",
        "# Tokenize datasets\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Set format for PyTorch\n",
        "tokenized_train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "tokenized_test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "# Create a DatasetDict\n",
        "datasets = DatasetDict({\n",
        "    'train': tokenized_train_dataset,\n",
        "    'val': tokenized_val_dataset,\n",
        "    'test': tokenized_test_dataset\n",
        "})\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=8,   # batch size for training\n",
        "    per_device_eval_batch_size=8,    # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "# Create Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=datasets['train'],\n",
        "    eval_dataset=datasets['val'],\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "GG5ygiB_pkZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **tuning model**"
      ],
      "metadata": {
        "id": "dQ3hiQ2Iqtej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch pandas scikit-learn"
      ],
      "metadata": {
        "id": "jx3t_lTOrWcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "import torch\n",
        "# Load the data\n",
        "train_data = pd.read_csv('cleaned_train.csv')\n",
        "val_data = pd.read_csv('cleaned_val.csv')\n",
        "test_data = pd.read_csv('cleaned_test.csv')\n",
        "\n",
        "# Display column names to ensure they are correct\n",
        "print(train_data.columns)\n",
        "print(val_data.columns)\n",
        "print(test_data.columns)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-fa-base-uncased\")\n",
        "\n",
        "# Tokenize data\n",
        "def tokenize_data(data, tokenizer):\n",
        "    texts = list(data.apply(lambda row: f\"{row['premise']} [SEP] {row['hypothesis']}\", axis=1))\n",
        "    return tokenizer(\n",
        "        texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "train_encodings = tokenize_data(train_data, tokenizer)\n",
        "val_encodings = tokenize_data(val_data, tokenizer)\n",
        "test_encodings = tokenize_data(test_data, tokenizer)\n",
        "\n",
        "# Ensure labels are mapped to integers\n",
        "label_mapping = {'e': 0, 'n': 1, 'c': 2}  # Replace this with the actual mapping\n",
        "train_labels = train_data['label'].map(label_mapping).tolist()\n",
        "val_labels = val_data['label'].map(label_mapping).tolist()\n",
        "test_labels = test_data['label'].map(label_mapping).tolist()\n"
      ],
      "metadata": {
        "id": "g9SPRvdQrehI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = CustomDataset(train_encodings, train_labels)\n",
        "val_dataset = CustomDataset(val_encodings, val_labels)\n",
        "test_dataset = CustomDataset(test_encodings, test_labels)\n"
      ],
      "metadata": {
        "id": "O25SNZPJrieY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"HooshvareLab/bert-fa-base-uncased\", num_labels=3)\n",
        "\n",
        "# Training settings\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch',\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")\n"
      ],
      "metadata": {
        "id": "IRnZBuDArlY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "K8lLdFH2rnvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate(eval_dataset=val_dataset)\n",
        "\n"
      ],
      "metadata": {
        "id": "MJSeOZadrqT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test set\n",
        "predictions = trainer.predict(test_dataset)\n",
        "# Extract true labels and predicted labels\n",
        "true_labels = test_labels\n",
        "predicted_labels = predictions.predictions.argmax(-1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "print(f\"F1 Score: {f1}\")\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
      ],
      "metadata": {
        "id": "pEk1RnkrrsZl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}